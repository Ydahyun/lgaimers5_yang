import numpy as np
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline

def calculate_fuzzy_membership(X, y):
    # numpy 배열로 변환
    X = np.array(X)
    y = np.array(y)
    
    # 클래스별로 데이터 분할
    unique_classes = np.unique(y)
    memberships = np.zeros(len(y))

    for cls in unique_classes:
        # 해당 클래스의 샘플만 선택
        class_indices = np.where(y == cls)[0]
        class_data = X[class_indices]
        
        # 클래스 내에서의 거리 계산 (간단히 유클리드 거리 사용)
        centroid = np.mean(class_data, axis=0)
        distances = np.linalg.norm(class_data - centroid, axis=1)
        
        # 거리를 기반으로 엔트로피 계산
        # 거리의 역수를 확률처럼 사용
        distances_prob = distances / np.sum(distances)
        entropy = -distances_prob * np.log(distances_prob + 1e-9)  # 로그의 0 처리
        entropy = np.sum(entropy)

        # 엔트로피를 멤버십 값으로 변환 (엔트로피가 낮을수록 멤버십 값이 높음)
        memberships[class_indices] = 1 / (1 + entropy)
    
    return memberships

class EFSVM:
    def __init__(self, C=1.0, kernel='rbf', gamma='scale'):
        self.C = C
        self.kernel = kernel
        self.gamma = gamma
        self.svm = None
    
    def fit(self, X, y, membership):
        # 가중치 부여
        sample_weight = membership

        # SVM 학습
        self.svm = make_pipeline(StandardScaler(), SVC(C=self.C, kernel=self.kernel, gamma=self.gamma))
        self.svm.fit(X, y, svc__sample_weight=sample_weight)

    def predict(self, X):
        return self.svm.predict(X)

# 데이터를 로드하고 전처리한 후

# numpy 배열로 변환 (필요한 경우)
X_res = np.array(X_res)
y_res = np.array(y_res)

# 퍼지 멤버십 계산
membership = calculate_fuzzy_membership(X_res, y_res)

# EFSVM 모델 학습
efsvm = EFSVM(C=1.0, kernel='rbf', gamma='scale')
efsvm.fit(X_res, y_res, membership)
